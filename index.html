<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      display: flex;
      justify-content: center;
      align-items: flex-start;
    }
    .container {
      width: 70%;
      padding: 2% 25% 2% 35%;
      text-align: left;
    }
    ul {
      list-style-type: disc;
    }
    ul ul {
      list-style-type: circle;
    }
    .links {
      margin-top: 20px;
    }
    .links a {
      margin-right: 15px;
    }
  </style>
</head>
<body>

<div class="container">
  <h1>Superalignment</h1>
  
  <div class="links">
    <a href="https://jrin771.github.io/">About Me</a> 
  </div>
  
<h2> Why did I make this?</h2> 
  <ul>  
    <li>OpenAI recently announced the formation of the <a href="https://openai.com/blog/introducing-superalignment">Superalignment team. </a></li> 
    <li>Their mission is to solve the core technical challenges of superintelligence alignment in four years. </li> 
    <li>Their approach is to build a roughly human-level automated alignment researcher that they can then scale up and use to iteratively align superintelligence. </li>
    <li>This team is now Ilya Sutskever's main research focus and has 20% of OpenAI's compute. </li>
    <li>Despite a <a href="https://spectrum.ieee.org/the-alignment-problem-openai ">handful </a> of media <a href="https://techcrunch.com/2023/07/05/openai-is-forming-a-new-team-to-bring-superintelligent-ai-under-control/ ">attention</a>, I don't see many people talking about this right now. </li> 
    <li>This is suprising to me. </li>
    <li>I believe this area needs <b>significantly</b> more attention; events like this don't happen often. </li>    
  </ul>  
<h2> Foreword (Please Read This If Nothing Else)</h2> 
  <ul>
    <li><b>So much is uncertain right now.</b> </li>
    <li>The error bars around future capabilities and scenarios are <b>very</b> wide.</li>
    <li>However, as I stated before, events like the superalignment team formation don't happen every day. </li> 
    <li>I believe it would be in the best interest of everyone to have a better understanding of this effort.</li>
    <li><b>Thus, from my (September 14th, 2023) currently external, unaffiliated perspective, here's what's going on:</b></li> 
  </ul> 
  
<h2> First off, what is alignment?</h2> 
  <ul>   
    <li>Here's a good definition from Paul Christiano, the former head of OpenAI's Alignmentt team <a href="https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6">(edited for cohesiveness)</a> </li>
    <li>"When I say an AI "A" is aligned with an operator "H", I mean: "A" is trying to do what "H" wants it to do. I use alignment as a statement about the motives of "A", not about it's knowledge or ability." </li> 
    <li>There are entire websites dedicated just for discussing AI Alignment, so I won't spend too much additional time here. However, here are three other points to know:</li> 
    <li>The humanities side of alignment (if we're trying to align an AI to human values) is the question: "who's values". This is <b>incredibly important to consider</b>, but superalignment appears to be focused on a subset of that question, which is "can we align it to ANY values at all?"</li>
    <li>As an add-on, although this doesn't further answer the question of "what is alignment?", it does bring up  </li>  
    <li>There's concerns of alignment taxes like, we made it worse by aligning, but RLHF and InstructGPT, the later of which I'll cover soon, actually make the models a lot more usable. Less orthogonal, more related. </li>
    <li>P!=NP PROBLEM OF OBSERVING VS PRODUCING, AND HOW THIS IS INFLUENCING THEIR EFFORTS AND THOUGHTS AROUND THIS. </li>
  </ul>
  
<h2> Second, what's superintelligence?</h2> 
  <ul>  
    <li>"AI systems much smarter than humans" - OpenAI</li> 
    <li>"And the kind of superintelligence weâ€™re picturing here is a system that is vastly smarter than humans, that could possibly execute much faster. You can run it a lot in parallel, it could collaborate with many, many copies of itself, so a truly vastly powerful system." - Jan Leike</li>
  </ul>
  
<h2> Thus, what's superalignment?</h2> 
  <ul>  
    <li>"How do we ensure AI systems much smarter than humans follow human intent?" - OpenAI </li>
  </ul> 
  
<h2> What are the core concepts driving this effort?</h2> 
  <ul>  
    <li>Honesty - Is it telling the truth? (This doesn't necessarily mean we know every neuron, just like we don't know every neuron of a person, but we know if they're telling the truth) </li>  
    <li>What is it?</li> 
    <li>Why is it important?</li> 
    <li>Why is it hard?</li> 
    
    <li>Interpretability- What is it doing?</li> 
    <li>What is it?</li> 
    <li>Why is it important?</li> 
    <li>Why is it hard?</li> 
    
    <li>Agreement from human experts</li> 
    <li>What is it? Let's say GPT-6 has finished training and it's ready to be shipped to make a lot of money but there isn't conclusive evidence that it's aligned. The team wait a few weeks for more tests to be done but then it still is inconclusive. </li>
    <li>Why is it important? Commercial pressure to release models is strong and decisiveness is crucial. </li> 
    <li>Why is it hard?</li> 
  
  </ul> 
<h2> Ok, so what is the actual structure of this effort in the niche details?</h2> 
  <ul>  
    <li>Here is (from an interview Jan Leike did) like the main strategy they're doing: </li> 
    <li>2 year plan: Reduce the problem to an engineering problem. I.e. discover the key ideas and just scale them up</li> 
    <li>3 year plan:Be mostly done with the ai alignment researcher (so that you can deploy it at scale and really ramp up)</li>  
    <li>Ok another point to explicitly mention is that there are very wide error bars. Perhaps capabilities go up, perhaps  </li>

  </ul>
<h2> Specific Approaches </h2> 
  <ul>  
       <li>Right now speculating about year 3 doesn't make sense, but for year 2 "here are the techniques" this is what I've seen referenced across lots of stuff.</li>
    <li> Additional Pages with significantly more detail will be added (and I hope to include perspectives from many, many people working on this.) </li>  
    
    <li>WebGPT</li>  
    <li>What is it:GPT-3 gets hooked up to google</li>
    <li>Why is it important/interesting/etc. on its own.: LLMs are known to hallucinate (make things up). If it could search and back up its claims, it could be more truthful and reliable.</li> 
    <li>How does it fit in with the "big picture": You can't do anything without truthfulness and reiability</li>

    
    <li>InstructGPT (Jan Is very bullish)</li> 
    <li>GPT-4 Reviewing GPT-2</li> 
    <li>Codex/Copilot/Coding AIs</li> 
    <li>Generalization of AI from easy problems to hard problems ("grokking"), this is what Collin Burns is doing more of like why does it jailbreak if not?</li>   

    
    <li>Collin Burns "Learning Latent Representations" </li> 
    <li>Language Models don't always say what they think</li>  

    <li>Debate strategies</li>

    <li>Recursive Reward Modeling</li> 

    <li></li>

    <li></li>
  </ul> 
<h2> List of Changes</h2> 
  <ul>  
    <li><b>This list exists to ensure accountability, transparency, and to provide value to current and future internet historians. Date format is Day-Month-Year (AD System)</b></li> 
  </ul> 
  
</div>
</body>
</html>
