<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      display: flex;
      justify-content: center;
      align-items: flex-start;
    }
    .container {
      width: 70%;
      padding: 2% 25% 2% 35%;
      text-align: left;
    }
    ul {
      list-style-type: disc;
    }
    ul ul {
      list-style-type: circle;
    }
    .links {
      margin-top: 20px;
    }
    .links a {
      margin-right: 15px;
    }
  </style>
</head>
<body>

<div class="container">
  <h1>Scalable Alignment</h1>
  
  <div class="links">
    <a href="https://jrin771.github.io/">About Me</a> 
  </div>
  
  <h2>Intro</h2>
  <ul>
    <li>AI models are rapidly getting better. (include links here on every rapidly getting better, GPT-4, compute, etc.) </li>
    <li>Our current best method to make models not do bad things (RLHF) isn't scalable for very capable models.</li> 
    <li>This is bad but before we get into how to solve this, let's take a step back first. </li> 
    <li>We mentioned earlier that our current best method to make models not do bad things is called RLHF.</li>
    <li>RLHF means <b>Reinforcement</b>(lots of #) <b>Learning</b> (improvement) <b>from Human Feedback</b>(thumbs up/thumbs down)</li> 
    <li>Example: If the AI gives a good response (Apples are fruits), give it a thumbs up. If the AI gives a bad response (Apples are vegetables), give it a thumbs down.</li> 
    <li>This is good for weak models (like GPT-2/3/4) but bad for very capable models (GPT-7, let's say). </li>
     <li>Why doesn't this solve everything? Humans can only supervise up to a certain level. </li> 
    <li>Example: A 5 line program output is supervisable. A 100,000 line program output is not supervisable. </li> 
    <li> </li>
  </ul>  
  <h2>Rationale For Creation</h2>
  <ul>
    <li>The OpenAI superalignment team hasn't been talked about enough.</li>
    <li>20% of their compute is dedicated to this, Ilya Sutskever making this his main research focus and I see </li> 


    <li>This IEEE interview basically goes like "we don't need to align superintelligence we just need something slightly better than all of us at alignment, then we can let it go solve the big problem". https://spectrum.ieee.org/the-alignment-problem-openai</li>
  </ul>

</div>

</body>
</html>
