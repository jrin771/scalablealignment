<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      display: flex;
      justify-content: center;
      align-items: flex-start;
    }
    .container {
      width: 70%;
      padding: 2% 25% 2% 35%;
      text-align: left;
    }
    ul {
      list-style-type: disc;
    }
    ul ul {
      list-style-type: circle;
    }
    ul li {
    margin-bottom: calc(var(--base-spacing) * 1.5);
    }
    .links {
      margin-top: 20px;
    }
    .links a {
      margin-right: 15px;
    }
  </style>
</head>
<body>

<div class="container">
  <h1>Superalignment</h1>
  
  <div class="links">
    <a href="https://jrin771.github.io/">About Me</a> 
  </div>
  
<h2> Why did I make this?</h2> 
  <ul>  
    <li>OpenAI recently announced the formation of the <a href="https://openai.com/blog/introducing-superalignment">Superalignment team. </a></li> 
    <li>Their mission is to solve the core technical challenges of superintelligence alignment in four years. </li> 
    <li>Their approach is to build a roughly human-level automated alignment researcher that they can then scale up and use to iteratively align superintelligence. </li>
    <li><b>This team is now Ilya Sutskever's main research focus and has 20% of OpenAI's compute.</b></li>
    <li>Despite a <a href="https://spectrum.ieee.org/the-alignment-problem-openai ">handful </a> of media <a href="https://techcrunch.com/2023/07/05/openai-is-forming-a-new-team-to-bring-superintelligent-ai-under-control/ ">attention</a>, I don't see many people talking about this right now. </li> 
    <li>This is suprising to me. </li>
    <li>I believe this area needs <b>significantly</b> more attention; events like this don't happen often. </li>    
  </ul>  
  
<h2> Foreword (Please Read This If Nothing Else)</h2> 
  <ul>
    <li><b>So much is uncertain right now.</b> </li>
    <li>The error bars around future capabilities and scenarios are <b>very</b> wide.</li>
    <li>However, as I stated before, events like the superalignment team formation don't happen every day. </li> 
    <li>I believe it would be in the best interest of everyone to have a better understanding of this effort.</li>
    <li><b>Thus, from my (September 14th, 2023) currently external, unaffiliated perspective, here's what's going on:</b></li> 
  </ul> 
  
<h2> First off, what is alignment?</h2> 
  <ul>   
    <li>Here's a good definition from Paul Christiano, the former head of OpenAI's Alignmentt team <a href="https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6">(edited for cohesiveness)</a> </li>
    <li>"When I say an AI "A" is aligned with an operator "H", I mean: "A" is trying to do what "H" wants it to do. I use alignment as a statement about the motives of "A", not about it's knowledge or ability." </li> 
    <li>Three additional items of importance:</li>
    <li><b>First,</b> if we're trying to align an AI to human values the question arises of who's values? This is <b>incredibly important to consider</b>, but superalignment appears to be focused on a subset of that question, which is "can we align it to ANY values at all?"</li>
    <li><b>Second,</b> it's easier to check solutions for superalignment than to solve superalignment. This is a property of P vs NP which you can read about more <a href="https://en.wikipedia.org/wiki/P_versus_NP_problem">here.</a> </li>
    <li><b>Third,</b> there's concern about alignment making systems worse; this is called an <a href="https://aligned.substack.com/p/three-alignment-taxes">"alignment tax".</a> Luckily, as I'll examine later, this doesn't seem to be the case, and in fact the opposite. </li>
  </ul>
  
<h2> Second, what's superintelligence?</h2> 
  <ul>  
    <li>"AI systems much smarter than humans" - OpenAI</li> 
    <li>"And the kind of superintelligence we’re picturing here is a system that is vastly smarter than humans, that could possibly execute much faster. You can run it a lot in parallel, it could collaborate with many, many copies of itself, so a truly vastly powerful system." - <a href="https://axrp.net/episode/2023/07/27/episode-24-superalignment-jan-leike.html#four-year-deadline">Jan Leike</a></li>
    <li>These are both vague but are the best I could find. This is an area for improvement, but I suspect it will only come from getting closer to "superintelligence".</li>
  </ul>
  
<h2> Thus, what's superalignment?</h2> 
  <ul>  
    <li>"How do we ensure AI systems much smarter than humans follow human intent?" - OpenAI </li>
  </ul> 
  
<h2> What are the core concepts driving this effort?</h2> 
  <ul>  
    <li><b>Honesty</b></li>
    <li><b>What is it?</b> Having the model be truthful to us about what it knows (and doesn't) and why it does actions (and why it doesn't).</li> 
    <li><b>Why is it important?</b> Deceitful AI with vast power could end up very badly for humans. This feels self-explanatory. </li> 
    <li><b>Why is it hard?</b> Sometimes the model doesn't know either. Current best approaches involve looking into the latent space of models, rather than taking them at face value (which we'll cover more) but this is still an area of basic research.</li> 
    
    <li><b>Interpretability</b></li> 
    <li><b>What is it?</b> Knowing what's going on inside the AI. This is different from honesty because it spans many size scales (from individual neuron to full model), but it is related. </li> 
    <li><b>Why is it important?</b> Without good ways of interpreting models, ensuring that they're not going to be dangerous is very hard.</li> 
    <li><b>Why is it hard?</b> Current Frontier Models are extremely large and complicated, somewhat like real brains. (Yes, I know the brain doesn't do backpropagation, it's just an analogy) Right now we can only interpert very small systems. We'll need to scale FAST. </li> 
    
    <li><b>Human Expert Agreement</b></li> 
    <li><b>What is it?</b> Let's say GPT-6 has finished training and it's ready to be shipped to make a lot of money but there isn't conclusive evidence that it's aligned. The team wait a few weeks for more tests to be done but then it still is inconclusive. What would you do?</li> 
    <li><b>Why is it important?</b> To avoid accidents born from commercial presures pulling in one direction and uncertainty pulling in the other.</li> 
    <li><b>Why is it hard?</b> The only way to avoid this disagreement is to either have Honesty or Interpretability work, which are both hard. </li> 
  </ul>
  
<h2> What's the medium-term strategy of this effort? </h2> 
  <ul>  
    <li>From <a href="https://axrp.net/episode/2023/07/27/episode-24-superalignment-jan-leike.html#four-year-deadline">an interview Jan Leike (Co-Lead of Superalignment)</a> did: </li> 
    <li><b>2-year goal:</b> Reduce superalignment to an engineering problem. i.e. discover the key ideas and just scale them up</li> 
    <li><b>3-year goal:</b> Be mostly done with the automated AI alignment researcher so that they can scale it up to "millions of copies". </li>  
  </ul>
  

<h2> Specific Approaches </h2>
<ul>
  <li>Right now nobody knows what's going to happen at Year 3/4, but for Year 2 here is a (non-exhaustive) list of promising techniques OpenAI has mentioned.</li>
  <li>Additional pages covering these techniques will be written in more detail (preferably with community assistance) soon.</li>
  <li><h3>WebGPT</h3>
    <ul>
      <li><b>What is it?</b>: WebGPT is an extension of GPT that incorporates a web-browsing capability. It can perform web searches to verify information before generating a response.</li>
      <li><b>Why is it important/interesting?</b>: It addresses the issue of hallucination and credibility. With web browsing, the model can fetch real-time data and verify facts, reducing erroneous claims.</li>
      <li><b>How does this fit in with "Automated AI Alignment Researcher"?</b>: In the context of alignment, a model that can verify its claims is closer to being aligned with human values for accuracy and truthfulness.</li>
    </ul>
  </li>
  <li><h3>InstructGPT (Jan Is very bullish)</h3>
    <ul>
      <li><b>What is it?</b>: InstructGPT is trained to follow a prompt's instruction more closely than vanilla GPT.</li>
      <li><b>Why is it important/interesting?</b>: It has higher task-specific performance and reduces irrelevant output, effectively generating more aligned responses.</li>
      <li><b>How does this fit in with "Automated AI Alignment Researcher"?</b>: Higher task alignment on a micro-level could be a building block for broader AI alignment strategies.</li>
    </ul>
  </li>
  <li><h3>GPT-4 Reviewing GPT-2</h3>
    <ul>
      <li><b>What is it?</b>: A hypothetical scenario where a more advanced model (GPT-4) evaluates the limitations and capabilities of a previous version (GPT-2).</li>
      <li><b>Why is it important/interesting?</b>: This self-assessment can offer insights into how models view their limitations, thus aiding in alignment research.</li>
      <li><b>How does this fit in with "Automated AI Alignment Researcher"?</b>: Understanding limitations transparently is critical for creating a meta-level alignment researcher.</li>
    </ul>
  <li><h3>Codex/Copilot/Coding AIs</h3>
    <ul>
      <li><b>What is it?</b>: These are AI systems designed to assist with coding tasks.</li>
      <li><b>Why is it important/interesting?</b>: They demonstrate real-world utility and offer a sandbox to explore alignment in a constrained, task-specific domain.</li>
      <li><b>How does this fit in with "Automated AI Alignment Researcher"?</b>: Insights gained from these narrower contexts can be generalized to broader alignment issues.</li>
    </ul>
  </li>
  <li><h3>Generalization ("grokking")</h3>
    <ul>
      <li><b>What is it?</b>: The study of how well AI models generalize from their training data to unseen but related tasks.</li>
      <li><b>Why is it important/interesting?</b>: AI systems need to generalize well to be useful and safe in real-world, dynamic environments.</li>
      <li><b>How does this fit in with "Automated AI Alignment Researcher"?</b>: A model that generalizes well is more likely to understand the abstract principles underlying alignment.</li>
    </ul>
  </li>
  <li><h3>Collin Burns "Learning Latent Representations"</h3>
    <ul>
      <li><b>What is it?</b>: Research into how models can learn underlying features or structures from data.</li>
      <li><b>Why is it important/interesting?</b>: Better representations can result in more accurate and insightful output.</li>
      <li><b>How does this fit in with "Automated AI Alignment Researcher"?</b>: More precise representations can aid in understanding complex alignment tasks.</li>
    </ul>
  </li>
  <li><h3>Language Models don't always say what they think</h3>
    <ul>
      <li><b>What is it?</b>: The observation that language models often produce outputs that may not fully reflect their "understanding" due to various biases and limitations.</li>
      <li><b>Why is it important/interesting?</b>: This is a key issue in alignment, as it raises questions about the model's internal states versus its outputs.</li>
      <li><b>How does this fit in with "Automated AI Alignment Researcher"?</b>: It highlights the need for interpretability and introspection in alignment research.</li>
    </ul>
  </li>
  <li><h3>Debate strategies</h3>
    <ul>
      <li><b>What is it?</b>: A method where multiple AI agents debate a topic to generate more accurate and nuanced outputs.</li>
      <li><b>Why is it important/interesting?</b>: It introduces adversarial dynamics to produce more robust and reliable answers.</li>
      <li><b>How does this fit in with "Automated AI Alignment Researcher"?</b>: Debate can be a tool for surfacing limitations and biases, thus aiding alignment.</li>
    </ul>
  </li>
  <li><h3>Recursive Reward Modeling</h3>
    <ul>
      <li><b>What is it?</b>: A process where models are trained to predict the rewards that a human would assign, and these predictions are used for training.</li>
      <li><b>Why is it important/interesting?</b>: It offers a way to align a model’s objectives with human-defined goals.</li>
      <li><b>How does this fit in with "Automated AI Alignment Researcher"?</b>: This directly aims at solving the alignment problem by fine-tuning objectives to be in line with human values.</li>
    </ul>
  </li>
  <li><h3>Self-Critiquing Models</h3>
    <ul>
      <li><b>What is it?</b>: A process where models are trained to predict the rewards that a human would assign, and these predictions are used for training.</li>
      <li><b>Why is it important/interesting?</b>: It offers a way to align a model’s objectives with human-defined goals.</li>
      <li><b>How does this fit in with "Automated AI Alignment Researcher"?</b>: This directly aims at solving the alignment problem by fine-tuning objectives to be in line with human values.</li>
    </ul>
  </li>
</ul>
  
<h2> List of Changes</h2> 
  <ul>  
    <li><b>This list exists to ensure accountability, transparency, and to provide value to current and future internet historians. Date format is Day-Month-Year (AD System)</b></li> 
  </ul> 
  
</div>
</body>
</html>
