<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      display: flex;
      justify-content: center;
      align-items: flex-start;
    }
    .container {
      width: 70%;
      padding: 2% 25% 2% 35%;
      text-align: left;
    }
    ul {
      list-style-type: disc;
    }
    ul ul {
      list-style-type: circle;
    }
    .links {
      margin-top: 20px;
    }
    .links a {
      margin-right: 15px;
    }
  </style>
</head>
<body>

<div class="container">
  <h1>Superalignment</h1>
  
  <div class="links">
    <a href="https://jrin771.github.io/">About Me</a> 
  </div>
  
<h2> Why did I make this?</h2> 
  <ul>  
    <li>OpenAI recently announced the formation of the Superalignment team. </li> 
    <li>Their mission is to solve the core technical challenges of superintelligence alignment in four years. </li> 
    <li>Their approach is to build a roughly human-level automated alignment researcher. </li>
    <li>They can then use vast amounts of compute to scale their efforts, and iteratively align superintelligence. </li>
    <li>This team is now Ilya Sutskever's main research focus and has 20% of OpenAI's compute with strong possibilities of obtaining more if needed. </li>
    <li>This announcement got some media attention and a couple of blog posts, but it seems like nobody's really talking about this now. </li>
    <li>I believe this area needs significantly more attention, since it's not every day something like this appears. </li>  
    <li>There is a non-zero probability that this team is misguided, everything is a stochastic parrot and we can go about our days. </li>
    <li>The common thread throughout all of my research on this topic is that everything right now is uncertain. </li>
    <li>The error bars around future capabilities and scenarios are <b>very</b>wide.</li>
    <li>However, let's explore the reasoning behind the superalignment effort to try and see what they're seeing. </li>
    <li>Here's what's going on (from my external, unaffiliated perspective): </li> 
  </ul> 
<h2> First off, what is alignment?</h2> 
  <ul>  
    <li>"When I say an AI A is aligned with an operator H, I mean: </li>
    <li>A is trying to do what H wants it to do.</li> 
    <li>I use alignment as a statement about the motives of the [model], not about [it's] knowledge or ability."</li> 
    <li>-Paul Christiano, Former Head of OpenAI Alignment (words in [] are replaced for cohesiveness) </li> 
    <li>As an add-on, although this doesn't further answer the question of "what is alignment?", it does  </li>
  </ul>
<h2> Second, what's superintelligence?</h2> 
  <ul>  
    <li>"AI systems much smarter than humans" - OpenAI</li>
    <li>Note, I think using "smarter" leads to bad mental pictures. Smarter, in this case, means more knowledge and ability than humans to achieve a given goal.  </li> 
    <li>(Some might complain that this is still quite vague, which I would agree with you on, but refer back to "Mindset")</li>
  </ul>
<h2> Thus, what's superalignment?</h2> 
  <ul>  
    <li>"How do we ensure AI systems much smarter than humans follow human intent?" - OpenAI </li>
  </ul> 
<h2> What are the core concepts driving this effort?</h2> 
  <ul>  
    <li>Honesty - Tell me if you know or don't know or why you're doing it? It doesn't mean "hey, you know why </li> 
    <li>Interpretability- What is it doing?</li> 
    <li>Agreement from experts - this is a big one mentioned by jan because you have squeezing of economics on one side, but also "we don't know" what do you do? Prefer as much certainty as possible</li>
    <li></li>
  
  
  
  
  </ul> 
<h2> Ok, so how do we do that? (The Good Part) </h2> 
  <ul>  
    <li>To start, each of these approaches are briefly summmarized. </li>
    <li> Additional Pages with significantly more detail will be added (and I hope to include perspectives from many, many people working on this.) </li>  
    
    <li>WebGPT</li>  
    <li>What is it:GPT-3 gets hooked up to google</li>
    <li>Why is it important/interesting/etc. on its own.: LLMs are known to hallucinate (make things up). If it could search and back up its claims, it could be more truthful and reliable.</li> 
    <li>How does it fit in with the "big picture": You can't do anything without truthfulness and reiability</li>

    
    <li>InstructGPT</li> 
    <li>GPT-4 Reviewing GPT-2</li> 
    <li>Codex/Copilot/Coding AIs</li> 
    <li>Generalization of AI from easy problems to hard problems ("grokking")</li>  
    <li></li>
  </ul>
</div>

</body>
</html>
