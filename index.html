<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      display: flex;
      justify-content: center;
      align-items: flex-start;
    }
    .container {
      width: 70%;
      padding: 2% 25% 2% 35%;
      text-align: left;
    }
    ul {
      list-style-type: disc;
    }
    ul ul {
      list-style-type: circle;
    }
    .links {
      margin-top: 20px;
    }
    .links a {
      margin-right: 15px;
    }
  </style>
</head>
<body>

<div class="container">
  <h1>Superalignment</h1>
  
  <div class="links">
    <a href="https://jrin771.github.io/">About Me</a> 
  </div>
  
<h2> Why did I make this?</h2> 
  <ul>  
    <li>OpenAI recently announced the formation of the Superalignment team. </li> 
    <li>Their mission is to solve the core technical challenges of superintelligence alignment in four years. </li> 
    <li>Their approach is to build a roughly human-level automated alignment researcher. </li>
    <li>They can then use vast amounts of compute to scale their efforts, and iteratively align superintelligence. </li>
    <li>This team is now Ilya Sutskever's main research focus and has 20% of OpenAI's compute with strong possibilities of obtaining more if needed. </li>
    <li>This announcement got some media attention and a couple of blog posts, but it seems like nobody's really talking about this now. </li>
    <li>I believe this area (whether correct or misguided) needs significantly more attention, since it's not every day something like this appears. </li> 
    <li>Here's what's going on (from my external, unaffiliated perspective): </li> 
  </ul> 
 <h2> Mindset</h2> 
  <ul>  
    <li>The common thread throughout all of my research on this topic is that everything right now is uncertain. </li>
    <li>The error bars around future capabilities and scenarios are <b>very</b>wide. k</li>
    <li>However, there are valid reasons to believe that this effort is not misguided. To understand, let's begin with the fundamentals.</li>
  </ul> 
<h2> First off, what is alignment?</h2> 
  <ul>  
    <li>"When I say an AI A is aligned with an operator H, I mean: </li>
    <li>A is trying to do what H wants it to do.</li> 
    <li>I use alignment as a statement about the motives of the [model], not about [it's] knowledge or ability."</li> 
    <li>-Paul Christiano, Former Head of OpenAI Alignment (words in [] are replaced for cohesiveness) </li>
  </ul>
<h2> Second, what's superintelligence?</h2> 
  <ul>  
    <li>"AI systems much smarter than humans" - OpenAI</li>
    <li>Note, I think using "smarter" leads to bad mental pictures. Smarter, in this case, means more knowledge and ability than humans to achieve a given goal.  </li> 
    <li>(Some might complain that this is still quite vague, which I would agree with you on, but refer back to "Mindset")</li>
  </ul>
<h2> Thus, what's superalignment?</h2> 
  <ul>  
    <li>"How do we ensure AI systems much smarter than humans follow human intent?" - OpenAI </li>
  </ul>
<h2> Ok, so how do we do that? </h2> 
  <ul>  
    <li>Seems unsure, nobody has a "here is 100% of all the low level details" but here are the approaches/mindsets/methods/everything that goes into this. </li>
    <li>Here is a list of 10 things with short summaries. Click on these links to go to very, very in-depth pages discussing them. (I want to keep this home page very clean and simple, like a 2 page executive summary) </li>
  </ul>
<h2> Call to action (or, "what can I do?") </h2> 
  <ul>  
    <li>I have no clue. I didn't htink I'd get this far.   </li>
  </ul>
</div>

</body>
</html>
