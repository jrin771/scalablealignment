<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      display: flex;
      justify-content: center;
      align-items: flex-start;
    }
    .container {
      width: 70%;
      padding: 2% 25% 2% 35%;
      text-align: left;
    }
    ul {
      list-style-type: disc;
    }
    ul ul {
      list-style-type: circle;
    }
    .links {
      margin-top: 20px;
    }
    .links a {
      margin-right: 15px;
    }
  </style>
</head>
<body>

<div class="container">
  <h1>Scalable Alignment</h1>
  
  <div class="links">
    <a href="https://jrin771.github.io/">About Me</a> 
  </div>
  
  <h2>Intro</h2>
  <ul>
    <li>AI models are getting better. </li>  
    <li>Our current main method of making sure these models don't do bad things (RLHF, explained momentarily) doesn't scale. </li> 
    <li>This is bad because if we can't judge if these models aren't going to break the law, cause harm, or other bad things, then that's very bad. </li>
     <li>Scalable Alignment is a field of research that aims to find scalable solutions to the AI Alignment problem. </li>
  </ul>  
  <h2>RLHF</h2>
  <ul>
    <li>RLHF means <b>Reinforcement</b>(lots of #) <b>Learning</b> (improvement) <b>from Human Feedback</b>(thumbs up/thumbs down)</li>
    <li>Example: If the AI gives a good response (Apples are fruits), give it a thumbs up. If the AI gives a bad response (Apples are vegetables), give it a thumbs down.</li>
    <li>Why doesn't this solve everything? Humans can only supervise up to a certain level. </li> 
    <li>Example: A 5 line program output is supervisable. A 100,000 line program output is not supervisable. </li> 
    <li></li>
  </ul>  
  

</div>

</body>
</html>
