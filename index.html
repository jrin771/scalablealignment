<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      display: flex;
      justify-content: center;
      align-items: flex-start;
    }
    .container {
      width: 70%;
      padding: 2% 25% 2% 35%;
      text-align: left;
    }
    ul {
      list-style-type: disc;
    }
    ul ul {
      list-style-type: circle;
    }
    .links {
      margin-top: 20px;
    }
    .links a {
      margin-right: 15px;
    }
  </style>
</head>
<body>

<div class="container">
  <h1>Scalable Alignment</h1>
  
  <div class="links">
    <a href="mailto:jrin@stanford.edu">Email</a> 
    <a href="https://twitter.com/jacobrintamaki">Twitter</a>
  </div>
  
  <h2>Intro</h2>
  <ul>
    <li>AI models are getting better. </li>  
    <li>Our current main method of making sure these models don't do bad things (RLHF, explained momentarily) doesn't scale. </li> 
    <li>This is bad because if we can't judge if these models aren't going to break the law, cause harm, or other bad things, then that's very bad. </li>
     <li>Scalable Alignment is a field of research that aims to find scalable solutions to the AI Alignment problem. </li>
  </ul>  
 
  

</div>

</body>
</html>
